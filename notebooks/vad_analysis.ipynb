{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this notebook for generating VAD results on a selected dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SK\\anaconda3\\envs\\vad-analysis\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n"
     ]
    }
   ],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "from pyannote.core import SlidingWindowFeature, Annotation, Segment\n",
    "from pyannote.metrics.detection import DetectionAccuracy, DetectionErrorRate\n",
    "from pyannote.audio.pipelines import VoiceActivityDetection\n",
    "from pyannote.audio import Model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from silero_vad import load_silero_vad, read_audio, get_speech_timestamps\n",
    "\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import torch\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing pre-trained models on DIHARD III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use pyannote, sign up for a auth token with hugging face first. Save the auth token in a config.yaml file to use it (below cell does that to load the auth token)\n",
    "\n",
    "Follow this [link](https://huggingface.co/pyannote/segmentation-3.0#requirements) for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pyannote/segmentation-3.0 usage\n",
    "with open(\"../configs/config.yaml\", 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "AUTH_TOKEN = config.get('AUTH_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath for DIHARD 3 eval data, change this\n",
    "DIHARD_FLAC_FILEPATH = \"../../original_dihard3_dataset/third_dihard_challenge_eval/data/flac/\"\n",
    "DIHARD_LAB_FILEPATH = \"../../original_dihard3_dataset/third_dihard_challenge_eval/data/sad/\"\n",
    "DIHARD_ENHANCED_FILEPATH = \"../../speech_enhanced_dihard3/MP-SENet_dihard3/eval\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `VADResults` class\n",
    "\n",
    "Generic VAD wrapper class to calculate metrics. Utilizes pyannote's underlying utils for the calculation of metrics. As long as other VAD models (e.g. Silero) handles generating speech timestamps and converting into an annotation object, the `VADResults` class should be able to generate relevant metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VADType(str, Enum):\n",
    "    SILERO = \"silero\"\n",
    "    PYANNOTE = \"pyannote\"\n",
    "\n",
    "def load_lab_file(lab_file_path):\n",
    "    lab_annotation = Annotation()\n",
    "    with open(lab_file_path, \"r\") as lab_file:\n",
    "        for line in lab_file:\n",
    "            start, end, label = line.strip().split()\n",
    "            if label == 'speech':  # Only marking speech segments\n",
    "                lab_annotation[Segment(float(start), float(end))] = \"speech\"\n",
    "    return lab_annotation\n",
    "\n",
    "def annotation_to_frame_labels(annotation, total_duration, frame_duration):\n",
    "    num_frames = int(total_duration / frame_duration)\n",
    "    labels = np.zeros(num_frames)\n",
    "\n",
    "    for segment in annotation.get_timeline():\n",
    "        start_frame = int(segment.start / frame_duration)\n",
    "        end_frame = int(segment.end / frame_duration)\n",
    "        labels[start_frame:end_frame] = 1\n",
    "\n",
    "    return labels\n",
    "\n",
    "@dataclass\n",
    "class VADResults:\n",
    "    sound_file_path: str\n",
    "    label_file_path: str\n",
    "\n",
    "    # metrics\n",
    "    detection_accuracy: float = 0.0\n",
    "    detection_error_rate_value: float = 0.0\n",
    "    missed_detection_rate: float = 0.0\n",
    "    false_alarm_rate: float = 0.0\n",
    "    roc_auc: float = 0.0\n",
    "\n",
    "    vad_result: SlidingWindowFeature = None\n",
    "\n",
    "    frame_duration = 0.01\n",
    "\n",
    "    def load_audio_and_vad(self, vad_model: VADType):\n",
    "        if vad_model == VADType.PYANNOTE:\n",
    "            model = Model.from_pretrained(\n",
    "                \"pyannote/segmentation-3.0\", use_auth_token=AUTH_TOKEN\n",
    "            )\n",
    "            pipeline = VoiceActivityDetection(segmentation=model)\n",
    "            HYPER_PARAMETERS = {\n",
    "                # remove speech regions shorter than that many seconds.\n",
    "                \"min_duration_on\": 0.0,\n",
    "                # fill non-speech regions shorter than that many seconds.\n",
    "                \"min_duration_off\": 0.0,\n",
    "            }\n",
    "            pipeline.instantiate(HYPER_PARAMETERS)\n",
    "            if torch.cuda.is_available():\n",
    "                pipeline.to(torch.device('cuda'))\n",
    "            self.vad_result = pipeline(self.sound_file_path)\n",
    "        elif vad_model == VADType.SILERO:\n",
    "            model = load_silero_vad()\n",
    "            wav = read_audio(self.sound_file_path)\n",
    "            speech_timestamps = get_speech_timestamps(\n",
    "                wav,\n",
    "                model,\n",
    "                return_seconds=True,  # Return speech timestamps in seconds (default is samples)\n",
    "            )\n",
    "            annotation = Annotation()\n",
    "            for ts in speech_timestamps:\n",
    "                # Create a segment for the start and end times\n",
    "                segment = Segment(ts['start'], ts['end'])\n",
    "                # Add the segment to the annotation with a label (e.g., \"speech\")\n",
    "                annotation[segment] = \"speech\"\n",
    "                self.vad_result = annotation\n",
    "\n",
    "    def calcMetrics(self):\n",
    "        # Step 2: Load ground truth labels from the .lab file\n",
    "        ground_truth = load_lab_file(self.label_file_path)\n",
    "        # print(\"Ground truth loaded:\", ground_truth)\n",
    "\n",
    "        detection_accuracy = DetectionAccuracy()\n",
    "\n",
    "        # Step 3: Initialize detection error rate metric\n",
    "        detection_error_rate = DetectionErrorRate()\n",
    "\n",
    "        # Step 4: Compute Detection Error Rate (DER)\n",
    "        self.detection_accuracy = detection_accuracy(ground_truth, self.vad_result)\n",
    "        # print(\"Detection accuracy:\", self.detection_accuracy)\n",
    "\n",
    "        self.detection_error_rate_value = detection_error_rate(\n",
    "            ground_truth, self.vad_result\n",
    "        )\n",
    "        # print(\"Detection error rate:\", self.detection_error_rate_value)\n",
    "\n",
    "\n",
    "        # Step 5: Compute missed detection and false alarm rates\n",
    "        detailed_metrics = detection_error_rate.compute_components(\n",
    "            ground_truth, self.vad_result\n",
    "        )\n",
    "        # print(\"Detailed metrics:\", detailed_metrics)\n",
    "\n",
    "        missed_detection_duration = detailed_metrics[\"miss\"]\n",
    "        false_alarm_duration = detailed_metrics[\"false alarm\"]\n",
    "        total_reference_duration = detailed_metrics[\"total\"]\n",
    "\n",
    "        self.missed_detection_rate = (\n",
    "            (missed_detection_duration / total_reference_duration) * 100\n",
    "            if total_reference_duration != 0\n",
    "            else 0\n",
    "        )\n",
    "        self.false_alarm_rate = (\n",
    "            (false_alarm_duration / total_reference_duration) * 100\n",
    "            if total_reference_duration != 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        # Step 6: Compute ROC-AUC score\n",
    "        total_duration = self.vad_result.get_timeline().extent().duration\n",
    "        ground_truth_labels = annotation_to_frame_labels(\n",
    "            ground_truth, total_duration, self.frame_duration\n",
    "        )\n",
    "        vad_labels = annotation_to_frame_labels(\n",
    "            self.vad_result, total_duration, self.frame_duration\n",
    "        )\n",
    "        self.roc_auc = roc_auc_score(ground_truth_labels, vad_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of test files and corresponding label files\n",
    "flac_files = [f for f in os.listdir(DIHARD_FLAC_FILEPATH) if f.endswith(\".flac\")]\n",
    "lab_files = [f for f in os.listdir(DIHARD_LAB_FILEPATH) if f.endswith(\".lab\")]\n",
    "enhanced_vad_wav_files = [f for f in os.listdir(DIHARD_ENHANCED_FILEPATH) if f.endswith(\".wav\")]\n",
    "\n",
    "# Ensure matching files\n",
    "flac_files.sort()\n",
    "lab_files.sort()\n",
    "enhanced_vad_wav_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lists to store results for different models\n",
    "results_pyannote_original = []\n",
    "results_pyannote_enhanced = []\n",
    "results_silero_original = []\n",
    "results_silero_enhanced = []\n",
    "\n",
    "# Define the VAD models\n",
    "vad_models = [\"pyannote\", \"silero\"]\n",
    "\n",
    "for (flac_file, enhanced_vad_file, lab_file) in zip(flac_files, enhanced_vad_wav_files, lab_files):\n",
    "    \n",
    "    # File paths\n",
    "    flac_path = os.path.join(DIHARD_FLAC_FILEPATH, flac_file)\n",
    "    enhanced_vad_path = os.path.join(DIHARD_ENHANCED_FILEPATH, enhanced_vad_file)\n",
    "    lab_path = os.path.join(DIHARD_LAB_FILEPATH, lab_file)\n",
    "\n",
    "    for vad_model in vad_models:\n",
    "        # Process original VAD\n",
    "        vad_result = VADResults(sound_file_path=flac_path, label_file_path=lab_path)\n",
    "        vad_result.load_audio_and_vad(vad_model=vad_model)\n",
    "        vad_result.calcMetrics()\n",
    "\n",
    "        result = {\n",
    "            'flac_file': flac_file,\n",
    "            'vad_model': vad_model,\n",
    "            'detection_accuracy': vad_result.detection_accuracy,\n",
    "            'detection_error_rate': vad_result.detection_error_rate_value,\n",
    "            'missed_detection_rate': vad_result.missed_detection_rate,\n",
    "            'false_alarm_rate': vad_result.false_alarm_rate,\n",
    "            'roc_auc': vad_result.roc_auc\n",
    "        }\n",
    "\n",
    "        if vad_model == \"pyannote\":\n",
    "            results_pyannote_original.append(result)\n",
    "        else:\n",
    "            results_silero_original.append(result)\n",
    "\n",
    "        print(f\"Appended VAD result for {flac_file} using {vad_model}\")\n",
    "\n",
    "        # Process enhanced VAD\n",
    "        enhanced_vad_result = VADResults(sound_file_path=enhanced_vad_path, label_file_path=lab_path)\n",
    "        enhanced_vad_result.load_audio_and_vad(vad_model=vad_model)\n",
    "        enhanced_vad_result.calcMetrics()\n",
    "\n",
    "        enhanced_result = {\n",
    "            'flac_file': enhanced_vad_file,\n",
    "            'vad_model': vad_model,\n",
    "            'detection_accuracy': enhanced_vad_result.detection_accuracy,\n",
    "            'detection_error_rate': enhanced_vad_result.detection_error_rate_value,\n",
    "            'missed_detection_rate': enhanced_vad_result.missed_detection_rate,\n",
    "            'false_alarm_rate': enhanced_vad_result.false_alarm_rate,\n",
    "            'roc_auc': enhanced_vad_result.roc_auc\n",
    "        }\n",
    "\n",
    "        if vad_model == \"pyannote\":\n",
    "            results_pyannote_enhanced.append(enhanced_result)\n",
    "        else:\n",
    "            results_silero_enhanced.append(enhanced_result)\n",
    "\n",
    "        print(f\"Appended Enhanced VAD result for {enhanced_vad_file} using {vad_model}\")\n",
    "\n",
    "# Convert lists to DataFrames and save as CSV files\n",
    "df_pyannote_original = pd.DataFrame(results_pyannote_original)\n",
    "df_pyannote_enhanced = pd.DataFrame(results_pyannote_enhanced)\n",
    "df_silero_original = pd.DataFrame(results_silero_original)\n",
    "df_silero_enhanced = pd.DataFrame(results_silero_enhanced)\n",
    "\n",
    "df_pyannote_original.to_csv(\"pyannote_original_vad_results.csv\", index=False)\n",
    "df_pyannote_enhanced.to_csv(\"pyannote_enhanced_vad_results.csv\", index=False)\n",
    "df_silero_original.to_csv(\"silero_original_vad_results.csv\", index=False)\n",
    "df_silero_enhanced.to_csv(\"silero_enhanced_vad_results.csv\", index=False)\n",
    "\n",
    "print(\"All 4 CSV files have been saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vad-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
